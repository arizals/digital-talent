{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\ichal\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\ichal\\anaconda3\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ichal\\anaconda3\\lib\\site-packages (from bs4) (4.7.1)\n",
      "Requirement already satisfied: six in c:\\users\\ichal\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\users\\ichal\\anaconda3\\lib\\site-packages (from nltk) (3.4.0.3)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\ichal\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (1.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4 nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ICHAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    " \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    " \n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    " \n",
    "from html.parser import HTMLParser\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "wnl = WordNetLemmatizer() \n",
    "stop = stopwords.words('english')\n",
    "stop.append(\"new\")\n",
    "stop.append(\"like\")\n",
    "stop.append(\"u\")\n",
    "stop.append(\"it'\")\n",
    "stop.append(\"'s\")\n",
    "stop.append(\"n't\")\n",
    "stop.append('mr.')\n",
    "stop = set(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From http://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html\n",
    "\n",
    "def tokenizer(text):\n",
    "\n",
    "    tokens_ = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "\n",
    "    tokens = []\n",
    "    for token_by_sent in tokens_:\n",
    "        tokens += token_by_sent\n",
    "\n",
    "    tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "    tokens = list(filter(lambda t: t not in punctuation, tokens))\n",
    "    tokens = list(filter(lambda t: t not in [u\"'s\", u\"n't\", u\"...\", u\"''\", u'``', u'\\u2014', u'\\u2026', u'\\u2013'], tokens))\n",
    "     \n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        token = wnl.lemmatize(token)\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    filtered_tokens = list(map(lambda token: token.lower(), filtered_tokens))\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(tokens, num):\n",
    "    return Counter(tokens).most_common(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_article_df(urls):\n",
    "    articles = []\n",
    "    for index, row in urls.iterrows():\n",
    "        try:\n",
    "            data=row['text'].strip().replace(\"'\", \"\")\n",
    "            data = strip_tags(data)\n",
    "            soup = BeautifulSoup(data)\n",
    "            data = soup.get_text()\n",
    "            data = data.encode('ascii', 'ignore').decode('ascii')\n",
    "            document = tokenizer(data)\n",
    "            top_3 = get_keywords(document, 3)\n",
    "          \n",
    "            unzipped = list(zip(*top_3))\n",
    "            kw= list(unzipped[0])\n",
    "            kw=\",\".join(str(x) for x in kw)\n",
    "            articles.append((kw, row['title'], row['pubdate']))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            #print data\n",
    "            #break\n",
    "            pass\n",
    "        #break\n",
    "    article_df = pd.DataFrame(articles, columns=['keywords', 'title', 'pubdate'])\n",
    "    return article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tocsv.csv')\n",
    "data = []\n",
    "for index, row in df.iterrows():\n",
    "    data.append((row['Title'], row['Permalink'], row['Date'], row['Content']))\n",
    "data_df = pd.DataFrame(data, columns=['title' ,'url', 'pubdate', 'text' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Driving Digital by Isaac Sacolick - a book review</td>\n",
       "      <td>http://ericbrown.com/driving-digital-isaac-sac...</td>\n",
       "      <td>20170906</td>\n",
       "      <td>&lt;img class=\"alignleft size-medium wp-image-975...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Data and Culture go hand in hand</td>\n",
       "      <td>http://ericbrown.com/?p=9757</td>\n",
       "      <td>-11130</td>\n",
       "      <td>Last week, I spent an afternoon talking to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Data Quality - The most important data dimension?</td>\n",
       "      <td>http://ericbrown.com/data-quality-most-importa...</td>\n",
       "      <td>20170918</td>\n",
       "      <td>&lt;img class=\"size-medium wp-image-9764 alignrig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Be pragmatic, not dogmatic</td>\n",
       "      <td>http://ericbrown.com/be-pragmatic-not-dogmatic...</td>\n",
       "      <td>20170928</td>\n",
       "      <td>&lt;img class=\"alignright size-medium wp-image-97...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>The Data Way</td>\n",
       "      <td>http://ericbrown.com/the-data-way.htm</td>\n",
       "      <td>20171003</td>\n",
       "      <td>&lt;img class=\"alignleft size-medium wp-image-977...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "143  Driving Digital by Isaac Sacolick - a book review   \n",
       "144                   Data and Culture go hand in hand   \n",
       "145  Data Quality - The most important data dimension?   \n",
       "146                         Be pragmatic, not dogmatic   \n",
       "147                                       The Data Way   \n",
       "\n",
       "                                                   url   pubdate  \\\n",
       "143  http://ericbrown.com/driving-digital-isaac-sac...  20170906   \n",
       "144                       http://ericbrown.com/?p=9757    -11130   \n",
       "145  http://ericbrown.com/data-quality-most-importa...  20170918   \n",
       "146  http://ericbrown.com/be-pragmatic-not-dogmatic...  20170928   \n",
       "147              http://ericbrown.com/the-data-way.htm  20171003   \n",
       "\n",
       "                                                  text  \n",
       "143  <img class=\"alignleft size-medium wp-image-975...  \n",
       "144  Last week, I spent an afternoon talking to the...  \n",
       "145  <img class=\"size-medium wp-image-9764 alignrig...  \n",
       "146  <img class=\"alignright size-medium wp-image-97...  \n",
       "147  <img class=\"alignleft size-medium wp-image-977...  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ICHAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ICHAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>title</th>\n",
       "      <th>pubdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data,big,culture</td>\n",
       "      <td>Building a Data Culture</td>\n",
       "      <td>20141118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data,data-driven,make</td>\n",
       "      <td>Note to Self - Don't say \"Data Driven\" Anymore</td>\n",
       "      <td>20141120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>captured,canon,titmouse</td>\n",
       "      <td>Foto Friday - Titmouse on the Feeder</td>\n",
       "      <td>20141121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mobility,organization,device</td>\n",
       "      <td>The Cloud - Gateway to Enterprise Mobility</td>\n",
       "      <td>20141121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data,center,agile</td>\n",
       "      <td>The Agile Data Center</td>\n",
       "      <td>20141124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       keywords  \\\n",
       "0              data,big,culture   \n",
       "1         data,data-driven,make   \n",
       "2       captured,canon,titmouse   \n",
       "3  mobility,organization,device   \n",
       "4             data,center,agile   \n",
       "\n",
       "                                            title   pubdate  \n",
       "0                         Building a Data Culture  20141118  \n",
       "1  Note to Self - Don't say \"Data Driven\" Anymore  20141120  \n",
       "2            Foto Friday - Titmouse on the Feeder  20141121  \n",
       "3      The Cloud - Gateway to Enterprise Mobility  20141121  \n",
       "4                           The Agile Data Center  20141124  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df = build_article_df(data_df)\n",
    "article_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_array=[]\n",
    "for index, row in article_df.iterrows():\n",
    "    keywords=row['keywords'].split(',')\n",
    "    for kw in keywords:\n",
    "        keywords_array.append((kw.strip(' '), row['keywords']))\n",
    "kw_df = pd.DataFrame(keywords_array).rename(columns={0:'keyword', 1:'keywords'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>data,big,culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>big</td>\n",
       "      <td>data,big,culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>culture</td>\n",
       "      <td>data,big,culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data</td>\n",
       "      <td>data,data-driven,make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data-driven</td>\n",
       "      <td>data,data-driven,make</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       keyword               keywords\n",
       "0         data       data,big,culture\n",
       "1          big       data,big,culture\n",
       "2      culture       data,big,culture\n",
       "3         data  data,data-driven,make\n",
       "4  data-driven  data,data-driven,make"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = kw_df.keywords.tolist()\n",
    "names = kw_df.keyword.tolist()\n",
    "\n",
    "document_array = []\n",
    "for item in document:\n",
    "    items = item.split(',')\n",
    "    document_array.append((items))\n",
    "\n",
    "occurrences = OrderedDict((name, OrderedDict((name, 0) for name in names)) for name in names)\n",
    "\n",
    "# Find the co-occurrences:\n",
    "for l in document_array:\n",
    "    for i in range(len(l)):\n",
    "        for item in l[:i] + l[i + 1:]:\n",
    "            occurrences[l[i]][item] += 1\n",
    "\n",
    "co_occur = pd.DataFrame.from_dict(occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>big</th>\n",
       "      <th>culture</th>\n",
       "      <th>may</th>\n",
       "      <th>skill</th>\n",
       "      <th>data-driven</th>\n",
       "      <th>make</th>\n",
       "      <th>company</th>\n",
       "      <th>decision</th>\n",
       "      <th>captured</th>\n",
       "      <th>...</th>\n",
       "      <th>manager</th>\n",
       "      <th>love</th>\n",
       "      <th>song</th>\n",
       "      <th>scene</th>\n",
       "      <th>isaac</th>\n",
       "      <th>quality</th>\n",
       "      <th>governance</th>\n",
       "      <th>pragmatic</th>\n",
       "      <th>dogmatic</th>\n",
       "      <th>thats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7d</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abstract</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>access</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>across</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>act</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 338 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          data  big  culture  may  skill  data-driven  make  company  \\\n",
       "7d           0    0        0    0      0            0     0        0   \n",
       "abstract     0    0        0    0      0            0     0        0   \n",
       "access       5    0        0    0      0            0     0        0   \n",
       "across       0    0        0    0      0            0     0        0   \n",
       "act          0    0        0    0      0            0     0        0   \n",
       "\n",
       "          decision  captured  ...  manager  love  song  scene  isaac  quality  \\\n",
       "7d               0        10  ...        0     0     0      0      0        0   \n",
       "abstract         0         0  ...        0     0     0      0      0        0   \n",
       "access           0         0  ...        0     0     0      0      0        0   \n",
       "across           0         0  ...        0     0     0      0      0        0   \n",
       "act              0         0  ...        0     0     0      0      0        0   \n",
       "\n",
       "          governance  pragmatic  dogmatic  thats  \n",
       "7d                 0          0         0      0  \n",
       "abstract           0          0         0      0  \n",
       "access             0          0         0      0  \n",
       "across             0          0         0      0  \n",
       "act                0          0         0      0  \n",
       "\n",
       "[5 rows x 338 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_occur.to_csv('ericbrown_co-occurancy_matrix.csv')\n",
    "co_occur.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 29),\n",
       " ('big', 8),\n",
       " ('culture', 8),\n",
       " ('may', 7),\n",
       " ('skill', 6),\n",
       " ('time', 6),\n",
       " ('corporate', 5),\n",
       " ('failure', 5),\n",
       " ('analysis', 5),\n",
       " ('many', 4),\n",
       " ('initiative', 4),\n",
       " ('scientist', 4),\n",
       " ('project', 4),\n",
       " ('system', 3),\n",
       " ('right', 3),\n",
       " ('people', 3),\n",
       " ('one', 3),\n",
       " ('analytics', 3),\n",
       " ('listen', 3),\n",
       " ('willing', 3),\n",
       " ('spent', 3),\n",
       " ('company', 2),\n",
       " ('want', 2),\n",
       " ('money', 2),\n",
       " ('software', 2),\n",
       " ('training', 2),\n",
       " ('able', 2),\n",
       " ('analyze', 2),\n",
       " ('use', 2),\n",
       " ('need', 2),\n",
       " ('area', 2),\n",
       " ('organization', 2),\n",
       " ('fail', 2),\n",
       " ('program', 2),\n",
       " ('involves', 2),\n",
       " ('listening', 2),\n",
       " ('investigation', 2),\n",
       " ('success', 2),\n",
       " ('doesnt', 2),\n",
       " ('information', 2),\n",
       " ('act', 2),\n",
       " ('important', 2),\n",
       " ('aspect', 2),\n",
       " ('dont', 2),\n",
       " ('example', 2),\n",
       " ('argument', 2),\n",
       " ('show', 2),\n",
       " ('cultural', 2),\n",
       " ('make', 2),\n",
       " ('well', 2),\n",
       " ('working', 2),\n",
       " ('arent', 2),\n",
       " ('finding', 2),\n",
       " ('value', 2),\n",
       " ('end', 2),\n",
       " ('much', 2),\n",
       " ('small', 2),\n",
       " ('today', 1),\n",
       " ('theyre', 1),\n",
       " ('spending', 1),\n",
       " ('consulting', 1),\n",
       " ('service', 1),\n",
       " ('capture', 1),\n",
       " ('process', 1),\n",
       " ('thing', 1),\n",
       " ('done', 1),\n",
       " ('science', 1),\n",
       " ('capability', 1),\n",
       " ('companies', 1),\n",
       " ('platform', 1),\n",
       " ('properly', 1),\n",
       " ('theres', 1),\n",
       " ('address', 1),\n",
       " ('building', 1),\n",
       " ('specifically', 1),\n",
       " ('around', 1),\n",
       " ('curiosity', 1),\n",
       " ('willingness', 1),\n",
       " ('try', 1),\n",
       " ('play', 1),\n",
       " ('huge', 1),\n",
       " ('role', 1),\n",
       " ('hearing', 1),\n",
       " ('provide', 1),\n",
       " ('conflicting', 1),\n",
       " ('set', 1),\n",
       " ('beginning', 1),\n",
       " ('experience', 1),\n",
       " ('ability', 1),\n",
       " ('lead', 1),\n",
       " ('leadership', 1),\n",
       " ('team', 1),\n",
       " ('ceo', 1),\n",
       " ('go', 1),\n",
       " ('belief', 1),\n",
       " ('difficult', 1),\n",
       " ('reality', 1),\n",
       " ('different', 1),\n",
       " ('expects', 1),\n",
       " ('accepting', 1),\n",
       " ('competing', 1),\n",
       " ('top', 1),\n",
       " ('issue', 1),\n",
       " ('break', 1),\n",
       " ('curious', 1),\n",
       " ('spend', 1),\n",
       " ('plenty', 1),\n",
       " ('investigating', 1),\n",
       " ('wasting', 1),\n",
       " ('giving', 1),\n",
       " ('proper', 1),\n",
       " ('become', 1),\n",
       " ('interested', 1),\n",
       " ('avenue', 1),\n",
       " ('get', 1),\n",
       " ('move', 1),\n",
       " ('lastly', 1),\n",
       " ('accept', 1),\n",
       " ('im', 1),\n",
       " ('saying', 1),\n",
       " ('embrace', 1),\n",
       " ('excuse', 1),\n",
       " ('world', 1),\n",
       " ('match', 1),\n",
       " ('expectation', 1),\n",
       " ('looking', 1),\n",
       " ('way', 1),\n",
       " ('look', 1),\n",
       " ('nothing', 1),\n",
       " ('measurable', 1),\n",
       " ('even', 1),\n",
       " ('though', 1),\n",
       " ('seem', 1),\n",
       " ('wasted', 1),\n",
       " ('type', 1),\n",
       " ('great', 1),\n",
       " ('allows', 1),\n",
       " ('continuously', 1),\n",
       " ('improve', 1),\n",
       " ('successfully', 1),\n",
       " ('implementing', 1),\n",
       " ('buying', 1),\n",
       " ('successful', 1),\n",
       " ('require', 1),\n",
       " ('soft', 1),\n",
       " ('organizational', 1),\n",
       " ('ensure', 1),\n",
       " ('mindset', 1),\n",
       " ('ingrained', 1),\n",
       " ('throughout', 1),\n",
       " ('post', 1),\n",
       " ('brought', 1),\n",
       " ('sas', 1)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# latihan saja\n",
    "text = data_df.iloc[0]['text']\n",
    "data = text.strip().replace(\"'\", \"\")\n",
    "data = strip_tags(data)\n",
    "soup = BeautifulSoup(data)\n",
    "data = soup.get_text()\n",
    "data = data.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "tokens_ = [word_tokenize(sent) for sent in sent_tokenize(data)]\n",
    "tokens = []\n",
    "for token_by_sent in tokens_:\n",
    "    tokens += token_by_sent\n",
    "\n",
    "tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "tokens = list(filter(lambda t: t not in punctuation, tokens))\n",
    "tokens = list(filter(lambda t: t not in [u\"'s\", u\"n't\", u\"...\", u\"''\", u'``', u'\\u2014', u'\\u2026', u'\\u2013'], tokens))\n",
    "\n",
    "filtered_tokens = []\n",
    "for token in tokens:\n",
    "    token = wnl.lemmatize(token)\n",
    "    if re.search('[a-zA-Z]', token):\n",
    "        filtered_tokens.append(token)\n",
    "\n",
    "filtered_tokens = list(map(lambda token: token.lower(), filtered_tokens))\n",
    "\n",
    "counts = Counter(filtered_tokens)\n",
    "\n",
    "sorted_counts = sorted(counts.items(), key=lambda count: count[1], reverse=True)\n",
    "sorted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 29 matches:\n",
      "many company want big data today theyre spending money system s\n",
      "ice able capture process analyze use data thing need done data science capabil\n",
      "ess analyze use data thing need done data science capability skill companies n\n",
      "ople skill able properly analyze use data theres one area many organization fa\n",
      "y organization fail address building data analytics program skill area involve\n",
      "lture play huge role success failure data analytics program company culture do\n",
      "ogram company culture doesnt hearing data may provide conflicting information \n",
      " provide conflicting information big data initiative may set failure beginning\n",
      "inning experience ability listen act data one important aspect corporate cultu\n",
      "spect corporate culture lead success data analytics big data dont corporate cu\n",
      "ture lead success data analytics big data dont corporate culture leadership te\n",
      "nformation example ceo doesnt listen data argument go belief may difficult tim\n",
      "rgument go belief may difficult time data analysis show reality different one \n",
      "stening accepting competing argument data top cultural issue make break big da\n",
      "ta top cultural issue make break big data cultural aspect important well examp\n",
      "mportant well example people working data arent curious data willing spend ple\n",
      "le people working data arent curious data willing spend plenty time investigat\n",
      "ling spend plenty time investigating data may wasting money giving people prop\n",
      "ey giving people proper skill become data scientist may training act data scie\n",
      "come data scientist may training act data scientist arent interested finding d\n",
      "a scientist arent interested finding data investigation avenue analysis may ge\n",
      "enue analysis may get move value big data initiative lastly corporate culture \n",
      "ing embrace excuse failure many time data analysis world end finding analysis \n",
      "nt match expectation much time spent data scientist spent small analysis proje\n",
      "ll analysis project looking way look data many small project end failure nothi\n"
     ]
    }
   ],
   "source": [
    "my_text = nltk.Text(filtered_tokens)\n",
    "my_text.concordance('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('big', 'data'), 0.027874564459930314),\n",
       " (('corporate', 'culture'), 0.017421602787456445),\n",
       " (('data', 'initiative'), 0.013937282229965157),\n",
       " (('data', 'scientist'), 0.013937282229965157),\n",
       " (('data', 'analytics'), 0.010452961672473868),\n",
       " (('act', 'data'), 0.006968641114982578),\n",
       " (('analytics', 'program'), 0.006968641114982578),\n",
       " (('analyze', 'use'), 0.006968641114982578),\n",
       " (('data', 'analysis'), 0.006968641114982578),\n",
       " (('data', 'may'), 0.006968641114982578),\n",
       " (('time', 'data'), 0.006968641114982578),\n",
       " (('time', 'spent'), 0.006968641114982578),\n",
       " (('use', 'data'), 0.006968641114982578),\n",
       " (('ability', 'listen'), 0.003484320557491289),\n",
       " (('able', 'capture'), 0.003484320557491289),\n",
       " (('able', 'properly'), 0.003484320557491289),\n",
       " (('accept', 'failure'), 0.003484320557491289),\n",
       " (('accepting', 'competing'), 0.003484320557491289),\n",
       " (('address', 'building'), 0.003484320557491289),\n",
       " (('allows', 'continuously'), 0.003484320557491289),\n",
       " (('analysis', 'dont'), 0.003484320557491289),\n",
       " (('analysis', 'may'), 0.003484320557491289),\n",
       " (('analysis', 'project'), 0.003484320557491289),\n",
       " (('analysis', 'show'), 0.003484320557491289),\n",
       " (('analysis', 'world'), 0.003484320557491289),\n",
       " (('analytics', 'big'), 0.003484320557491289),\n",
       " (('area', 'involves'), 0.003484320557491289),\n",
       " (('area', 'many'), 0.003484320557491289),\n",
       " (('arent', 'curious'), 0.003484320557491289),\n",
       " (('arent', 'interested'), 0.003484320557491289),\n",
       " (('argument', 'data'), 0.003484320557491289),\n",
       " (('argument', 'go'), 0.003484320557491289),\n",
       " (('around', 'listening'), 0.003484320557491289),\n",
       " (('aspect', 'corporate'), 0.003484320557491289),\n",
       " (('aspect', 'important'), 0.003484320557491289),\n",
       " (('avenue', 'analysis'), 0.003484320557491289),\n",
       " (('become', 'data'), 0.003484320557491289),\n",
       " (('beginning', 'experience'), 0.003484320557491289),\n",
       " (('belief', 'may'), 0.003484320557491289),\n",
       " (('break', 'big'), 0.003484320557491289),\n",
       " (('brought', 'sas'), 0.003484320557491289),\n",
       " (('building', 'data'), 0.003484320557491289),\n",
       " (('buying', 'software'), 0.003484320557491289),\n",
       " (('capability', 'skill'), 0.003484320557491289),\n",
       " (('capture', 'process'), 0.003484320557491289),\n",
       " (('ceo', 'doesnt'), 0.003484320557491289),\n",
       " (('companies', 'need'), 0.003484320557491289),\n",
       " (('company', 'culture'), 0.003484320557491289),\n",
       " (('company', 'want'), 0.003484320557491289),\n",
       " (('competing', 'argument'), 0.003484320557491289),\n",
       " (('conflicting', 'information'), 0.003484320557491289),\n",
       " (('consulting', 'training'), 0.003484320557491289),\n",
       " (('continuously', 'improve'), 0.003484320557491289),\n",
       " (('cultural', 'aspect'), 0.003484320557491289),\n",
       " (('cultural', 'issue'), 0.003484320557491289),\n",
       " (('culture', 'around'), 0.003484320557491289),\n",
       " (('culture', 'doesnt'), 0.003484320557491289),\n",
       " (('culture', 'ensure'), 0.003484320557491289),\n",
       " (('culture', 'lead'), 0.003484320557491289),\n",
       " (('culture', 'leadership'), 0.003484320557491289),\n",
       " (('culture', 'play'), 0.003484320557491289),\n",
       " (('culture', 'specifically'), 0.003484320557491289),\n",
       " (('culture', 'willing'), 0.003484320557491289),\n",
       " (('curiosity', 'investigation'), 0.003484320557491289),\n",
       " (('curious', 'data'), 0.003484320557491289),\n",
       " (('data', 'arent'), 0.003484320557491289),\n",
       " (('data', 'argument'), 0.003484320557491289),\n",
       " (('data', 'cultural'), 0.003484320557491289),\n",
       " (('data', 'dont'), 0.003484320557491289),\n",
       " (('data', 'investigation'), 0.003484320557491289),\n",
       " (('data', 'many'), 0.003484320557491289),\n",
       " (('data', 'mindset'), 0.003484320557491289),\n",
       " (('data', 'one'), 0.003484320557491289),\n",
       " (('data', 'science'), 0.003484320557491289),\n",
       " (('data', 'theres'), 0.003484320557491289),\n",
       " (('data', 'thing'), 0.003484320557491289),\n",
       " (('data', 'today'), 0.003484320557491289),\n",
       " (('data', 'top'), 0.003484320557491289),\n",
       " (('data', 'willing'), 0.003484320557491289),\n",
       " (('different', 'one'), 0.003484320557491289),\n",
       " (('difficult', 'time'), 0.003484320557491289),\n",
       " (('doesnt', 'hearing'), 0.003484320557491289),\n",
       " (('doesnt', 'listen'), 0.003484320557491289),\n",
       " (('done', 'data'), 0.003484320557491289),\n",
       " (('dont', 'corporate'), 0.003484320557491289),\n",
       " (('dont', 'match'), 0.003484320557491289),\n",
       " (('embrace', 'excuse'), 0.003484320557491289),\n",
       " (('end', 'failure'), 0.003484320557491289),\n",
       " (('end', 'finding'), 0.003484320557491289),\n",
       " (('ensure', 'big'), 0.003484320557491289),\n",
       " (('even', 'though'), 0.003484320557491289),\n",
       " (('example', 'ceo'), 0.003484320557491289),\n",
       " (('example', 'people'), 0.003484320557491289),\n",
       " (('excuse', 'failure'), 0.003484320557491289),\n",
       " (('expectation', 'much'), 0.003484320557491289),\n",
       " (('expects', 'want'), 0.003484320557491289),\n",
       " (('experience', 'ability'), 0.003484320557491289),\n",
       " (('fail', 'address'), 0.003484320557491289),\n",
       " (('fail', 'corporate'), 0.003484320557491289),\n",
       " (('failure', 'beginning'), 0.003484320557491289),\n",
       " (('failure', 'data'), 0.003484320557491289),\n",
       " (('failure', 'im'), 0.003484320557491289),\n",
       " (('failure', 'many'), 0.003484320557491289),\n",
       " (('failure', 'nothing'), 0.003484320557491289),\n",
       " (('finding', 'analysis'), 0.003484320557491289),\n",
       " (('finding', 'data'), 0.003484320557491289),\n",
       " (('get', 'move'), 0.003484320557491289),\n",
       " (('giving', 'people'), 0.003484320557491289),\n",
       " (('go', 'belief'), 0.003484320557491289),\n",
       " (('great', 'data'), 0.003484320557491289),\n",
       " (('hearing', 'data'), 0.003484320557491289),\n",
       " (('huge', 'role'), 0.003484320557491289),\n",
       " (('im', 'saying'), 0.003484320557491289),\n",
       " (('implementing', 'big'), 0.003484320557491289),\n",
       " (('important', 'aspect'), 0.003484320557491289),\n",
       " (('important', 'well'), 0.003484320557491289),\n",
       " (('improve', 'skill'), 0.003484320557491289),\n",
       " (('information', 'big'), 0.003484320557491289),\n",
       " (('information', 'example'), 0.003484320557491289),\n",
       " (('ingrained', 'throughout'), 0.003484320557491289),\n",
       " (('initiative', 'lastly'), 0.003484320557491289),\n",
       " (('initiative', 'may'), 0.003484320557491289),\n",
       " (('initiative', 'much'), 0.003484320557491289),\n",
       " (('initiative', 'require'), 0.003484320557491289),\n",
       " (('interested', 'finding'), 0.003484320557491289),\n",
       " (('investigating', 'data'), 0.003484320557491289),\n",
       " (('investigation', 'avenue'), 0.003484320557491289),\n",
       " (('investigation', 'willingness'), 0.003484320557491289),\n",
       " (('involves', 'corporate'), 0.003484320557491289),\n",
       " (('involves', 'culture'), 0.003484320557491289),\n",
       " (('issue', 'make'), 0.003484320557491289),\n",
       " (('lastly', 'corporate'), 0.003484320557491289),\n",
       " (('lead', 'success'), 0.003484320557491289),\n",
       " (('leadership', 'team'), 0.003484320557491289),\n",
       " (('listen', 'act'), 0.003484320557491289),\n",
       " (('listen', 'data'), 0.003484320557491289),\n",
       " (('listen', 'information'), 0.003484320557491289),\n",
       " (('listening', 'accepting'), 0.003484320557491289),\n",
       " (('listening', 'curiosity'), 0.003484320557491289),\n",
       " (('look', 'data'), 0.003484320557491289),\n",
       " (('looking', 'way'), 0.003484320557491289),\n",
       " (('make', 'break'), 0.003484320557491289),\n",
       " (('make', 'great'), 0.003484320557491289),\n",
       " (('many', 'company'), 0.003484320557491289),\n",
       " (('many', 'organization'), 0.003484320557491289),\n",
       " (('many', 'small'), 0.003484320557491289),\n",
       " (('many', 'time'), 0.003484320557491289),\n",
       " (('match', 'expectation'), 0.003484320557491289),\n",
       " (('may', 'difficult'), 0.003484320557491289),\n",
       " (('may', 'get'), 0.003484320557491289),\n",
       " (('may', 'provide'), 0.003484320557491289),\n",
       " (('may', 'seem'), 0.003484320557491289),\n",
       " (('may', 'set'), 0.003484320557491289),\n",
       " (('may', 'training'), 0.003484320557491289),\n",
       " (('may', 'wasting'), 0.003484320557491289),\n",
       " (('measurable', 'value'), 0.003484320557491289),\n",
       " (('mindset', 'ingrained'), 0.003484320557491289),\n",
       " (('money', 'giving'), 0.003484320557491289),\n",
       " (('money', 'system'), 0.003484320557491289),\n",
       " (('move', 'value'), 0.003484320557491289),\n",
       " (('much', 'buying'), 0.003484320557491289),\n",
       " (('much', 'time'), 0.003484320557491289),\n",
       " (('need', 'done'), 0.003484320557491289),\n",
       " (('need', 'right'), 0.003484320557491289),\n",
       " (('nothing', 'measurable'), 0.003484320557491289),\n",
       " (('one', 'area'), 0.003484320557491289),\n",
       " (('one', 'expects'), 0.003484320557491289),\n",
       " (('one', 'important'), 0.003484320557491289),\n",
       " (('organization', 'fail'), 0.003484320557491289),\n",
       " (('organization', 'post'), 0.003484320557491289),\n",
       " (('organizational', 'culture'), 0.003484320557491289),\n",
       " (('people', 'proper'), 0.003484320557491289),\n",
       " (('people', 'skill'), 0.003484320557491289),\n",
       " (('people', 'working'), 0.003484320557491289),\n",
       " (('platform', 'right'), 0.003484320557491289),\n",
       " (('play', 'huge'), 0.003484320557491289),\n",
       " (('plenty', 'time'), 0.003484320557491289),\n",
       " (('post', 'brought'), 0.003484320557491289),\n",
       " (('process', 'analyze'), 0.003484320557491289),\n",
       " (('program', 'company'), 0.003484320557491289),\n",
       " (('program', 'skill'), 0.003484320557491289),\n",
       " (('project', 'end'), 0.003484320557491289),\n",
       " (('project', 'even'), 0.003484320557491289),\n",
       " (('project', 'looking'), 0.003484320557491289),\n",
       " (('project', 'make'), 0.003484320557491289),\n",
       " (('proper', 'skill'), 0.003484320557491289),\n",
       " (('properly', 'analyze'), 0.003484320557491289),\n",
       " (('provide', 'conflicting'), 0.003484320557491289),\n",
       " (('reality', 'different'), 0.003484320557491289),\n",
       " (('require', 'working'), 0.003484320557491289),\n",
       " (('right', 'people'), 0.003484320557491289),\n",
       " (('right', 'platform'), 0.003484320557491289),\n",
       " (('right', 'system'), 0.003484320557491289),\n",
       " (('role', 'success'), 0.003484320557491289),\n",
       " (('saying', 'embrace'), 0.003484320557491289),\n",
       " (('science', 'capability'), 0.003484320557491289),\n",
       " (('scientist', 'allows'), 0.003484320557491289),\n",
       " (('scientist', 'arent'), 0.003484320557491289),\n",
       " (('scientist', 'may'), 0.003484320557491289),\n",
       " (('scientist', 'spent'), 0.003484320557491289),\n",
       " (('seem', 'wasted'), 0.003484320557491289),\n",
       " (('service', 'able'), 0.003484320557491289),\n",
       " (('set', 'failure'), 0.003484320557491289),\n",
       " (('show', 'reality'), 0.003484320557491289),\n",
       " (('show', 'time'), 0.003484320557491289),\n",
       " (('skill', 'able'), 0.003484320557491289),\n",
       " (('skill', 'area'), 0.003484320557491289),\n",
       " (('skill', 'become'), 0.003484320557491289),\n",
       " (('skill', 'companies'), 0.003484320557491289),\n",
       " (('skill', 'successfully'), 0.003484320557491289),\n",
       " (('skill', 'well'), 0.003484320557491289),\n",
       " (('small', 'analysis'), 0.003484320557491289),\n",
       " (('small', 'project'), 0.003484320557491289),\n",
       " (('soft', 'skill'), 0.003484320557491289),\n",
       " (('software', 'consulting'), 0.003484320557491289),\n",
       " (('software', 'system'), 0.003484320557491289),\n",
       " (('specifically', 'involves'), 0.003484320557491289),\n",
       " (('spend', 'plenty'), 0.003484320557491289),\n",
       " (('spending', 'money'), 0.003484320557491289),\n",
       " (('spent', 'data'), 0.003484320557491289),\n",
       " (('spent', 'project'), 0.003484320557491289),\n",
       " (('spent', 'small'), 0.003484320557491289),\n",
       " (('success', 'data'), 0.003484320557491289),\n",
       " (('success', 'failure'), 0.003484320557491289),\n",
       " (('successful', 'big'), 0.003484320557491289),\n",
       " (('successfully', 'implementing'), 0.003484320557491289),\n",
       " (('system', 'right'), 0.003484320557491289),\n",
       " (('system', 'software'), 0.003484320557491289),\n",
       " (('system', 'successful'), 0.003484320557491289),\n",
       " (('team', 'willing'), 0.003484320557491289),\n",
       " (('theres', 'one'), 0.003484320557491289),\n",
       " (('theyre', 'spending'), 0.003484320557491289),\n",
       " (('thing', 'need'), 0.003484320557491289),\n",
       " (('though', 'may'), 0.003484320557491289),\n",
       " (('throughout', 'organization'), 0.003484320557491289),\n",
       " (('time', 'investigating'), 0.003484320557491289),\n",
       " (('time', 'type'), 0.003484320557491289),\n",
       " (('today', 'theyre'), 0.003484320557491289),\n",
       " (('top', 'cultural'), 0.003484320557491289),\n",
       " (('training', 'act'), 0.003484320557491289),\n",
       " (('training', 'service'), 0.003484320557491289),\n",
       " (('try', 'fail'), 0.003484320557491289),\n",
       " (('type', 'project'), 0.003484320557491289),\n",
       " (('value', 'big'), 0.003484320557491289),\n",
       " (('value', 'show'), 0.003484320557491289),\n",
       " (('want', 'big'), 0.003484320557491289),\n",
       " (('want', 'listening'), 0.003484320557491289),\n",
       " (('wasted', 'time'), 0.003484320557491289),\n",
       " (('wasting', 'money'), 0.003484320557491289),\n",
       " (('way', 'look'), 0.003484320557491289),\n",
       " (('well', 'example'), 0.003484320557491289),\n",
       " (('well', 'organizational'), 0.003484320557491289),\n",
       " (('willing', 'accept'), 0.003484320557491289),\n",
       " (('willing', 'listen'), 0.003484320557491289),\n",
       " (('willing', 'spend'), 0.003484320557491289),\n",
       " (('willingness', 'try'), 0.003484320557491289),\n",
       " (('working', 'data'), 0.003484320557491289),\n",
       " (('working', 'soft'), 0.003484320557491289),\n",
       " (('world', 'end'), 0.003484320557491289)]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import BigramCollocationFinder\n",
    "finder = BigramCollocationFinder.from_words(filtered_tokens)\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big data today; use data thing; done data science; use data theres;\n",
      "building data analytics; failure data analytics; hearing data may; big\n",
      "data initiative; act data one; success data analytics; big data dont;\n",
      "listen data argument; time data analysis; argument data top; big data\n",
      "cultural; working data arent; curious data willing; investigating data\n",
      "may; become data scientist; act data scientist; finding data\n",
      "investigation; big data initiative; time data analysis; spent data\n",
      "scientist; look data many; great data scientist; big data initiative;\n",
      "big data initiative; big data mindset\n"
     ]
    }
   ],
   "source": [
    "my_text = nltk.Text(filtered_tokens)\n",
    "my_text.findall('<.*><data><.*>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
